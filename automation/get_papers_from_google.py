# import time
# import pickle
# import requests
# import os
# import re
# import random
# from urllib.parse import urlencode, urlparse, parse_qs
# from bs4 import BeautifulSoup

# # === CONFIG ===
# API_KEY = "fbb15dc82c0f4f0fd1d943c96104c114"
# SITE_KEY = "6LfFDwUTAAAAAIyC8IeC3aGLqVpvrB6ZpkfmAibj"
# BASE_URL = "https://scholar.google.com/scholar"
# COOKIE_FILE = "scholar_cookies.pkl"

# # User-Agent rotation list
# USER_AGENTS = [
#     'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
#     'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
#     'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
#     'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
#     'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
#     'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/121.0'
# ]

# def get_random_headers():
#     """Get random headers for requests"""
#     return {
#         'User-Agent': random.choice(USER_AGENTS),
#         'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
#         'Accept-Language': 'en-US,en;q=0.9',
#         'Accept-Encoding': 'gzip, deflate, br',
#         'DNT': '1',
#         'Connection': 'keep-alive',
#         'Upgrade-Insecure-Requests': '1',
#         'Sec-Fetch-Dest': 'document',
#         'Sec-Fetch-Mode': 'navigate',
#         'Sec-Fetch-Site': 'none',
#         'Sec-Fetch-User': '?1',
#         'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
#         'sec-ch-ua-mobile': '?0',
#         'sec-ch-ua-platform': '"Windows"'
#     }

# # === 2CAPTCHA SOLUTION ===
# def solve_fastcaptcha_dataportal(web_url, site_key, api_key, session, is_invisible=False, timeout=180, poll_interval=5):
#     """
#     Solve CAPTCHA using 2Captcha API with requests session
#     """
#     create_url = "https://api.2captcha.com/createTask"
#     result_url = "https://api.2captcha.com/getTaskResult"

#     # Get cookies from session
#     cookies_str = "; ".join([f"{name}={value}" for name, value in session.cookies.items()])
    
#     task_payload = {
#         "clientKey": api_key,
#         "task": {
#             "type": "RecaptchaV2TaskProxyless",
#             "websiteURL": web_url,
#             "websiteKey": site_key,
#             "isInvisible": bool(is_invisible),
#             "cookies": cookies_str 
#         }
#     }
#     print(f"ğŸ§  CAPTCHA task payload: {task_payload}")

#     # Create task
#     print("ğŸ§  Requesting CAPTCHA solution from 2Captcha (createTask)...")
#     create_resp = requests.post(create_url, json=task_payload, headers={"Content-Type": "application/json"})
    
#     if create_resp.status_code != 200:
#         raise Exception(f"âŒ 2Captcha createTask HTTP error: {create_resp.status_code} - {create_resp.text}")

#     create_json = create_resp.json()
#     if create_json.get("errorId") != 0:
#         raise Exception(f"âŒ 2Captcha createTask error: {create_json}")

#     task_id = create_json.get("taskId")
#     if not task_id:
#         raise Exception(f"âŒ 2Captcha createTask missing taskId: {create_json}")

#     # Poll for result
#     started = time.time()
#     while True:
#         if time.time() - started > timeout:
#             raise TimeoutError("âŒ› 2Captcha timeout waiting for solution.")

#         time.sleep(poll_interval)
#         check_payload = {
#             "clientKey": api_key,
#             "taskId": task_id
#         }
#         check_resp = requests.post(result_url, json=check_payload, headers={"Content-Type": "application/json"})
#         print(f"ğŸ” Checking task status: {check_resp.json()}")
        
#         if check_resp.status_code != 200:
#             raise Exception(f"âŒ 2Captcha getTaskResult HTTP error: {check_resp.status_code} - {check_resp.text}")

#         check_json = check_resp.json()
#         if check_json.get("errorId") != 0:
#             raise Exception(f"âŒ 2Captcha getTaskResult error: {check_json}")

#         status = check_json.get("status")
#         if status == "processing":
#             continue

#         if status == "ready":
#             time.sleep(10)
#             solution = check_json.get("solution", {})
#             token = solution.get("gRecaptchaResponse")
#             if not token:
#                 raise Exception(f"âŒ 2Captcha ready but no gRecaptchaResponse: {check_json}")
#             print("âœ… CAPTCHA solved via 2Captcha.")
#             return token

#         # Unexpected status
#         raise Exception(f"âŒ 2Captcha unknown status: {check_json}")

# # === UTILITY FUNCTIONS ===
# def build_scholar_url(query, start=0, start_year=None, end_year=None):
#     """Build Google Scholar URL with parameters"""
#     params = {
#         'q': query,
#         'hl': 'en',
#         'start': start,
#         'as_sdt': '0,50',
#         'as_rr': 1
#     }
#     if start_year:
#         params['as_ylo'] = start_year
#     if end_year:
#         params['as_yhi'] = end_year
#     return BASE_URL + '?' + urlencode(params)

# def is_captcha_present(html_content):
#     """Check if CAPTCHA is present in the HTML content"""
#     return "recaptcha" in html_content.lower() or "are you a robot" in html_content.lower()

# def save_cookies(session, path=COOKIE_FILE):
#     """Save session cookies to file"""
#     with open(path, "wb") as file:
#         pickle.dump(dict(session.cookies), file)
#         print("ğŸª Cookies saved.")

# def load_cookies(session, path=COOKIE_FILE):
#     """Load cookies from file into session"""
#     if not os.path.exists(path):
#         return
    
#     try:
#         with open(path, "rb") as file:
#             cookies = pickle.load(file)
#             for name, value in cookies.items():
#                 session.cookies.set(name, value, domain='.google.com')
#         print("ğŸª Cookies loaded.")
#     except Exception as e:
#         print(f"âš ï¸ Error loading cookies: {e}")

# def handle_captcha_verification(session, current_url, token):
#     """Handle CAPTCHA verification by making request with token"""
#     # Parse current URL and add CAPTCHA response parameter
#     parsed_url = urlparse(current_url)
#     query_params = parse_qs(parsed_url.query)
    
#     # Flatten query params (parse_qs returns lists)
#     flat_params = {k: v[0] if isinstance(v, list) and v else v for k, v in query_params.items()}
#     flat_params['g-recaptcha-response'] = token
    
#     # Rebuild URL with CAPTCHA response
#     captcha_url = f"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}?" + urlencode(flat_params)
    
#     print(f"ğŸ”— Making CAPTCHA verification request to: {captcha_url}")
    
#     # Make request with CAPTCHA token
#     headers = get_random_headers()
#     response = session.get(captcha_url, headers=headers, timeout=30)
    
#     if response.status_code == 200:
#         save_cookies(session)
#         print("ğŸª Cookies saved after CAPTCHA verification")
#         return response
#     else:
#         raise Exception(f"âŒ CAPTCHA verification failed: {response.status_code}")

# def parse_scholar_results(html_content):
#     """Parse Google Scholar results from HTML content"""
#     soup = BeautifulSoup(html_content, 'html.parser')
#     results = []
    
#     # Find all article containers
#     articles = soup.find_all('div', class_=['gs_r', 'gs_or', 'gs_scl']) or soup.find_all('div', {'data-lid': True})
    
#     for article in articles:
#         try:
#             # Extract title
#             title_tag = article.find('h3', class_='gs_rt') or article.find('a')
#             if not title_tag:
#                 continue
#             title = title_tag.get_text().strip()
            
#             # Extract PDF link
#             pdf_link = None
#             pdf_tags = article.find_all('a', href=True)
#             for pdf_tag in pdf_tags:
#                 href = pdf_tag.get('href', '')
#                 if href.lower().endswith('.pdf') or 'pdf' in pdf_tag.get_text().lower():
#                     pdf_link = href
#                     break
            
#             # Extract author info
#             author_tag = article.find('div', class_='gs_a')
#             author_info = author_tag.get_text().strip() if author_tag else ""
            
#             # Extract year using regex
#             year_match = re.search(r'\b(19|20)\d{2}\b', author_info)
#             year = year_match.group(0) if year_match else None
            
#             # Extract cited by information
#             cited_by = None
#             cited_links = article.find_all('a', href=True)
#             for a in cited_links:
#                 text = a.get_text().lower()
#                 if "cited by" in text:
#                     cited_by = re.sub(r'[^\d]', '', a.get_text())
#                     break
            
#             # Only keep if it's a PDF link
#             if pdf_link and pdf_link.lower().endswith(".pdf"):
#                 results.append({
#                     "title": title,
#                     "pdf_link": pdf_link,
#                     "author_info": author_info,
#                     "year": year,
#                     "cited_by": cited_by
#                 })
#                 print("âœ… Article added:", title)
                
#         except Exception as e:
#             print(f"âš ï¸ Error parsing article: {e}")
#             continue
    
#     return results

# def get_random_proxy():
#     """Get random proxy (kept from original code)"""
#     try:
#         import urllib.request
#         resp = urllib.request.urlopen("http://list.didsoft.com/get?email=tikuntechnologies@gmail.com&pass=bwnh68&pid=http1000&showcountry=no&level=1&country=US")
#         data = resp.read().decode('utf-8').strip()
#         urls = data.split("\n")
#         return random.choice(urls) 
#     except Exception as e:
#         print(f"âš ï¸ Error getting proxy: {e}")
#         return None

# # === MAIN SCRAPER ===
# def scrape_scholar_pages(query, start_year, end_year):
#     """Main scraping function using requests"""
#     print("ğŸ” Starting Google Scholar scraping with requests...")
    
#     # Create session for persistent cookies
#     session = requests.Session()
    
#     # Configure session with retry strategy
#     from requests.adapters import HTTPAdapter
#     from urllib3.util.retry import Retry
    
#     retry_strategy = Retry(
#         total=3,
#         backoff_factor=1,
#         status_forcelist=[429, 500, 502, 503, 504],
#     )
#     adapter = HTTPAdapter(max_retries=retry_strategy)
#     session.mount("http://", adapter)
#     session.mount("https://", adapter)
    
#     # Load existing cookies if available
#     load_cookies(session)
    
#     all_results = []
    
#     for start in [0, 10, 20]:  # Scrape 3 pages
#         try:
#             url = build_scholar_url(query, start, start_year, end_year)
#             print(f"\nğŸŒ Making request to: {url}")
            
#             # Random delay between requests
#             time.sleep(random.uniform(2, 5))
            
#             # Make request with random headers
#             headers = get_random_headers()
#             response = session.get(url, headers=headers, timeout=30)
            
#             if response.status_code != 200:
#                 print(f"âŒ HTTP Error {response.status_code}: {response.text[:500]}")
#                 continue
            
#             # Check for CAPTCHA
#             if is_captcha_present(response.text):
#                 print("ğŸ›‘ CAPTCHA detected. Solving...")
#                 try:
#                     token = solve_fastcaptcha_dataportal(url, SITE_KEY, API_KEY, session)
#                     response = handle_captcha_verification(session, url, token)
#                 except Exception as captcha_error:
#                     print(f"âŒ CAPTCHA solving failed: {captcha_error}")
#                     continue
            
#             # Parse results
#             page_results = parse_scholar_results(response.text)
#             all_results.extend(page_results)
            
#             print(f"ğŸ“„ Page {start//10 + 1}: Found {len(page_results)} PDF articles")
            
#         except requests.RequestException as e:
#             print(f"âŒ Request error for start={start}: {e}")
#             continue
#         except Exception as e:
#             print(f"âŒ General error for start={start}: {e}")
#             continue
    
#     print(f"\nğŸ‰ Scraping complete! Total PDF articles found: {len(all_results)}")
#     return all_results


import time
import pickle
import requests
import os
import re
import random
from urllib.parse import urlencode, urlparse, parse_qs
from bs4 import BeautifulSoup

# === CONFIG ===
API_KEY = "fbb15dc82c0f4f0fd1d943c96104c114"
SITE_KEY = "6LfFDwUTAAAAAIyC8IeC3aGLqVpvrB6ZpkfmAibj"
BASE_URL = "https://scholar.google.com/scholar"
COOKIE_FILE = "scholar_cookies.pkl"

# More diverse User-Agent rotation list
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 OPR/106.0.0.0'
]

# Referer options for Google Scholar
REFERERS = [
    'https://www.google.com/',
    'https://scholar.google.com/',
    'https://www.google.com/search?q=google+scholar',
    'https://en.wikipedia.org/',
    'https://www.researchgate.net/'
]

def get_random_headers():
    """Get random headers for requests with better anti-detection"""
    return {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'Accept-Language': 'en-US,en;q=0.9',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'same-origin' if random.random() > 0.5 else 'cross-site',
        'Sec-Fetch-User': '?1',
        'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': f'"{random.choice(["Windows", "macOS", "Linux"])}"',
        'Cache-Control': 'max-age=0',
        'Referer': random.choice(REFERERS)
    }

# === 2CAPTCHA SOLUTION ===
def solve_fastcaptcha_dataportal(web_url, site_key, api_key, session, is_invisible=False, timeout=180, poll_interval=5):
    """
    Solve CAPTCHA using 2Captcha API with requests session
    """
    create_url = "http://api.2captcha.com/createTask"
    result_url = "http://api.2captcha.com/getTaskResult"

    # Get cookies from session
    cookies_str = "; ".join([f"{name}={value}" for name, value in session.cookies.items()])
    
    task_payload = {
        "clientKey": api_key,
        "task": {
            "type": "RecaptchaV2TaskProxyless",
            "websiteURL": web_url,
            "websiteKey": site_key,
            "isInvisible": bool(is_invisible),
            "cookies": cookies_str 
        }
    }
    print(f"ğŸ§  CAPTCHA task payload: {task_payload}")

    # Create task
    print("ğŸ§  Requesting CAPTCHA solution from 2Captcha (createTask)...")
    create_resp = requests.post(create_url, json=task_payload, headers={"Content-Type": "application/json"})
    
    if create_resp.status_code != 200:
        raise Exception(f"âŒ 2Captcha createTask HTTP error: {create_resp.status_code} - {create_resp.text}")

    create_json = create_resp.json()
    if create_json.get("errorId") != 0:
        raise Exception(f"âŒ 2Captcha createTask error: {create_json}")

    task_id = create_json.get("taskId")
    if not task_id:
        raise Exception(f"âŒ 2Captcha createTask missing taskId: {create_json}")

    # Poll for result
    started = time.time()
    while True:
        if time.time() - started > timeout:
            raise TimeoutError("âŒ› 2Captcha timeout waiting for solution.")

        time.sleep(poll_interval)
        check_payload = {
            "clientKey": api_key,
            "taskId": task_id
        }
        check_resp = requests.post(result_url, json=check_payload, headers={"Content-Type": "application/json"})
        print(f"ğŸ” Checking task status: {check_resp.json()}")
        
        if check_resp.status_code != 200:
            raise Exception(f"âŒ 2Captcha getTaskResult HTTP error: {check_resp.status_code} - {check_resp.text}")

        check_json = check_resp.json()
        if check_json.get("errorId") != 0:
            raise Exception(f"âŒ 2Captcha getTaskResult error: {check_json}")

        status = check_json.get("status")
        if status == "processing":
            continue

        if status == "ready":
            time.sleep(10)
            solution = check_json.get("solution", {})
            token = solution.get("gRecaptchaResponse")
            if not token:
                raise Exception(f"âŒ 2Captcha ready but no gRecaptchaResponse: {check_json}")
            print("âœ… CAPTCHA solved via 2Captcha.")
            return token

        # Unexpected status
        raise Exception(f"âŒ 2Captcha unknown status: {check_json}")

# === UTILITY FUNCTIONS ===
def build_scholar_url(query, start=0, start_year=None, end_year=None):
    """Build Google Scholar URL with parameters"""
    params = {
        'q': query,
        'hl': 'en',
        'start': start,
        'as_sdt': '0,50',
        'as_rr': 1
    }
    if start_year:
        params['as_ylo'] = start_year
    if end_year:
        params['as_yhi'] = end_year
    return BASE_URL + '?' + urlencode(params)

def is_captcha_present(html_content):
    """Check if CAPTCHA is present in the HTML content"""
    return "recaptcha" in html_content.lower() or "are you a robot" in html_content.lower()

def save_cookies(session, path=COOKIE_FILE):
    """Save session cookies to file"""
    with open(path, "wb") as file:
        pickle.dump(dict(session.cookies), file)
        print("ğŸª Cookies saved.")

def load_cookies(session, path=COOKIE_FILE):
    """Load cookies from file into session"""
    if not os.path.exists(path):
        return
    
    try:
        with open(path, "rb") as file:
            cookies = pickle.load(file)
            for name, value in cookies.items():
                session.cookies.set(name, value, domain='.google.com')
        print("ğŸª Cookies loaded.")
    except Exception as e:
        print(f"âš ï¸ Error loading cookies: {e}")

def handle_captcha_verification(session, current_url, token):
    """Handle CAPTCHA verification by making request with token"""
    # Parse current URL and add CAPTCHA response parameter
    parsed_url = urlparse(current_url)
    query_params = parse_qs(parsed_url.query)
    
    # Flatten query params (parse_qs returns lists)
    flat_params = {k: v[0] if isinstance(v, list) and v else v for k, v in query_params.items()}
    flat_params['g-recaptcha-response'] = token
    
    # Rebuild URL with CAPTCHA response
    captcha_url = f"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}?" + urlencode(flat_params)
    
    print(f"ğŸ”— Making CAPTCHA verification request to: {captcha_url}")
    
    # Make request with CAPTCHA token
    headers = get_random_headers()
    response = session.get(captcha_url, headers=headers, timeout=30)
    
    if response.status_code == 200:
        save_cookies(session)
        print("ğŸª Cookies saved after CAPTCHA verification")
        return response
    else:
        raise Exception(f"âŒ CAPTCHA verification failed: {response.status_code}")

def parse_scholar_results(html_content):
    """Parse Google Scholar results from HTML content"""
    soup = BeautifulSoup(html_content, 'html.parser')
    results = []
    
    # Find all article containers
    articles = soup.find_all('div', class_=['gs_r', 'gs_or', 'gs_scl']) or soup.find_all('div', {'data-lid': True})
    
    for article in articles:
        try:
            # Extract title
            title_tag = article.find('h3', class_='gs_rt') or article.find('a')
            if not title_tag:
                continue
            title = title_tag.get_text().strip()
            
            # Extract PDF link
            pdf_link = None
            pdf_tags = article.find_all('a', href=True)
            for pdf_tag in pdf_tags:
                href = pdf_tag.get('href', '')
                if href.lower().endswith('.pdf') or 'pdf' in pdf_tag.get_text().lower():
                    pdf_link = href
                    break
            
            # Extract author info
            author_tag = article.find('div', class_='gs_a')
            author_info = author_tag.get_text().strip() if author_tag else ""
            
            # Extract year using regex
            year_match = re.search(r'\b(19|20)\d{2}\b', author_info)
            year = year_match.group(0) if year_match else None
            
            # Extract cited by information
            cited_by = None
            cited_links = article.find_all('a', href=True)
            for a in cited_links:
                text = a.get_text().lower()
                if "cited by" in text:
                    cited_by = re.sub(r'[^\d]', '', a.get_text())
                    break
            
            # Only keep if it's a PDF link
            if pdf_link and pdf_link.lower().endswith(".pdf"):
                results.append({
                    "title": title,
                    "pdf_link": pdf_link,
                    "author_info": author_info,
                    "year": year,
                    "cited_by": cited_by
                })
                print("âœ… Article added:", title)
                
        except Exception as e:
            print(f"âš ï¸ Error parsing article: {e}")
            continue
    
    return results

def make_initial_request(session):
    """Make an initial request to Google Scholar homepage to establish session"""
    try:
        print("ğŸ  Making initial request to Google Scholar homepage...")
        headers = get_random_headers()
        headers['Referer'] = 'https://www.google.com/'
        
        # Visit Google first
        google_resp = session.get('https://www.google.com/', headers=headers, timeout=10)
        time.sleep(random.uniform(1, 3))
        
        # Then visit Scholar homepage
        scholar_resp = session.get('https://scholar.google.com/', headers=headers, timeout=10)
        
        if scholar_resp.status_code == 200:
            print("âœ… Successfully established session with Google Scholar")
            save_cookies(session)
            return True
        else:
            print(f"âš ï¸ Scholar homepage returned: {scholar_resp.status_code}")
            return False
            
    except Exception as e:
        print(f"âš ï¸ Error establishing session: {e}")
        return False

def handle_403_error(session, url, attempt=1, max_attempts=3):
    """Handle 403 errors with progressive backoff and session reset"""
    if attempt > max_attempts:
        return None
        
    print(f"ğŸ›‘ 403 Error - Attempt {attempt}/{max_attempts}")
    
    # Progressive delays
    delay = random.uniform(5 * attempt, 15 * attempt)
    print(f"â³ Waiting {delay:.1f} seconds...")
    time.sleep(delay)
    
    # Reset session on subsequent attempts
    if attempt > 1:
        session.cookies.clear()
        if not make_initial_request(session):
            return None
    
    # Try with different approach
    try:
        headers = get_random_headers()
        
        # Add some randomization to headers
        if random.random() > 0.7:
            headers.pop('DNT', None)
        if random.random() > 0.8:
            headers.pop('Upgrade-Insecure-Requests', None)
            
        response = session.get(url, headers=headers, timeout=30)
        
        if response.status_code == 200:
            print("âœ… Successfully recovered from 403 error")
            return response
        elif response.status_code == 403:
            return handle_403_error(session, url, attempt + 1, max_attempts)
        else:
            print(f"âŒ Got status code {response.status_code} on retry")
            return None
            
    except Exception as e:
        print(f"âŒ Error on retry attempt {attempt}: {e}")
        if attempt < max_attempts:
            return handle_403_error(session, url, attempt + 1, max_attempts)
        return None
    """Get random proxy (kept from original code)"""
    try:
        import urllib.request
        resp = urllib.request.urlopen("http://list.didsoft.com/get?email=tikuntechnologies@gmail.com&pass=bwnh68&pid=http1000&showcountry=no&level=1&country=US")
        data = resp.read().decode('utf-8').strip()
        urls = data.split("\n")
        return random.choice(urls) 
    except Exception as e:
        print(f"âš ï¸ Error getting proxy: {e}")
        return None

# === MAIN SCRAPER ===
def scrape_scholar_pages(query, start_year, end_year):
    """Main scraping function using requests"""
    print("ğŸ” Starting Google Scholar scraping with requests...")
    
    # Create session for persistent cookies
    session = requests.Session()
    
    # Optional: Setup proxy (uncomment if needed)
    # setup_proxy_session(session)
    
    # Configure session with retry strategy
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
    
    retry_strategy = Retry(
        total=2,
        backoff_factor=2,
        status_forcelist=[500, 502, 503, 504],
        allowed_methods=["GET"]
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    # Load existing cookies if available
    load_cookies(session)
    
    # Establish initial session
    if not make_initial_request(session):
        print("âš ï¸ Failed to establish initial session, proceeding anyway...")
    
    all_results = []
    
    for start in [0, 10, 20]:  # Scrape 3 pages
        try:
            url = build_scholar_url(query, start, start_year, end_year)
            print(f"\nğŸŒ Making request to: {url}")
            
            # Longer random delay between requests
            delay = random.uniform(5, 12)
            print(f"â³ Waiting {delay:.1f} seconds before request...")
            time.sleep(delay)
            
            # Make request with random headers
            headers = get_random_headers()
            response = session.get(url, headers=headers, timeout=30)
            
            # Handle different status codes
            if response.status_code == 403:
                print("ğŸ›‘ Got 403 error, attempting recovery...")
                response = handle_403_error(session, url)
                if not response:
                    print(f"âŒ Failed to recover from 403 error for start={start}")
                    continue
                    
            elif response.status_code == 429:
                print("ğŸ›‘ Rate limited (429), waiting longer...")
                time.sleep(random.uniform(30, 60))
                continue
                
            elif response.status_code != 200:
                print(f"âŒ HTTP Error {response.status_code}: {response.text[:500]}")
                continue
            
            # Check for CAPTCHA
            if is_captcha_present(response.text):
                print("ğŸ›‘ CAPTCHA detected. Solving...")
                try:
                    token = solve_fastcaptcha_dataportal(url, SITE_KEY, API_KEY, session)
                    response = handle_captcha_verification(session, url, token)
                except Exception as captcha_error:
                    print(f"âŒ CAPTCHA solving failed: {captcha_error}")
                    continue
            
            # Parse results
            page_results = parse_scholar_results(response.text)
            all_results.extend(page_results)
            
            print(f"ğŸ“„ Page {start//10 + 1}: Found {len(page_results)} PDF articles")
            
        except requests.RequestException as e:
            print(f"âŒ Request error for start={start}: {e}")
            continue
        except Exception as e:
            print(f"âŒ General error for start={start}: {e}")
            continue
    
    print(f"\nğŸ‰ Scraping complete! Total PDF articles found: {len(all_results)}")
    return all_results
